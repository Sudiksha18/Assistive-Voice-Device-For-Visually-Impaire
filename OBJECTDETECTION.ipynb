{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf3a00a2-8ad7-4fce-ae2b-14018dcf05b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading YOLO from disk...\n",
      "Detections: ['mid  center  person with 1.00 accuracy, from x axis is 3 meters, from y axis is 3 meters']\n",
      "Average Accuracy: 99.96\n",
      "Detections: ['mid  center  person with 0.99 accuracy, from x axis is 3 meters, from y axis is 3 meters']\n",
      "Average Accuracy: 99.32\n",
      "Detections: ['mid  center  person with 1.00 accuracy, from x axis is 3 meters, from y axis is 3 meters']\n",
      "Average Accuracy: 99.94\n",
      "Detections: ['mid  center  person with 1.00 accuracy, from x axis is 4 meters, from y axis is 3 meters']\n",
      "Average Accuracy: 99.94\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import cv2\n",
    "import os\n",
    "import subprocess\n",
    "from gtts import gTTS \n",
    "from pydub import AudioSegment\n",
    "\n",
    "# Set the path to your ffmpeg binary\n",
    "AudioSegment.converter = \"ffmpeg.exe\"  # Replace with the correct path\n",
    "\n",
    "# Load the COCO class labels our YOLO model was trained on\n",
    "LABELS = open(\"coco.names\").read().strip().split(\"\\n\")\n",
    "\n",
    "# Load our YOLO object detector trained on COCO dataset (80 classes)\n",
    "print(\"[INFO] loading YOLO from disk...\")\n",
    "net = cv2.dnn.readNetFromDarknet(\"yolov3.cfg\", \"yolov3.weights\")\n",
    "\n",
    "# Determine only the output layer names that we need from YOLO\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "frame_count = 0\n",
    "start = time.time()\n",
    "frames = []\n",
    "\n",
    "while True:\n",
    "    frame_count += 1\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    frames.append(frame)\n",
    "\n",
    "    if frame_count == 300:\n",
    "        break\n",
    "    if ret:\n",
    "        key = cv2.waitKey(1)\n",
    "        if frame_count % 60 == 0:\n",
    "            end = time.time()\n",
    "            # Grab the frame dimensions and convert it to a blob\n",
    "            (H, W) = frame.shape[:2]\n",
    "            # Construct a blob from the input image and then perform a forward\n",
    "            # pass of the YOLO object detector, giving us our bounding boxes and\n",
    "            # associated probabilities\n",
    "            blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416),\n",
    "                swapRB=True, crop=False)\n",
    "            net.setInput(blob)\n",
    "            layerOutputs = net.forward(output_layers)\n",
    "\n",
    "            # Initialize our lists of detected bounding boxes, confidences, and\n",
    "            # class IDs, respectively\n",
    "            boxes = []\n",
    "            confidences = []\n",
    "            classIDs = []\n",
    "            centers = []\n",
    "\n",
    "            # Loop over each of the layer outputs\n",
    "            for output in layerOutputs:\n",
    "                # Loop over each of the detections\n",
    "                for detection in output:\n",
    "                    # Extract the class ID and confidence (i.e., probability) of\n",
    "                    # the current object detection\n",
    "                    scores = detection[5:]\n",
    "                    classID = np.argmax(scores)\n",
    "                    confidence = scores[classID]\n",
    "\n",
    "                    # Filter out weak predictions by ensuring the detected\n",
    "                    # probability is greater than the minimum probability\n",
    "                    if confidence > 0.5:\n",
    "                        # Scale the bounding box coordinates back relative to the\n",
    "                        # size of the image, keeping in mind that YOLO actually\n",
    "                        # returns the center (x, y)-coordinates of the bounding\n",
    "                        # box followed by the boxes' width and height\n",
    "                        box = detection[0:4] * np.array([W, H, W, H])\n",
    "                        (centerX, centerY, width, height) = box.astype(\"int\")\n",
    "\n",
    "                        # Use the center (x, y)-coordinates to derive the top and\n",
    "                        # and left corner of the bounding box\n",
    "                        x = int(centerX - (width / 2))\n",
    "                        y = int(centerY - (height / 2))\n",
    "\n",
    "                        # Update our list of bounding box coordinates, confidences,\n",
    "                        # and class IDs\n",
    "                        boxes.append([x, y, int(width), int(height)])\n",
    "                        confidences.append(float(confidence))\n",
    "                        classIDs.append(classID)\n",
    "                        centers.append((centerX, centerY))\n",
    "\n",
    "            # Apply non-maxima suppression to suppress weak, overlapping bounding\n",
    "            # boxes\n",
    "            idxs = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.3)\n",
    "\n",
    "            texts = []\n",
    "            total_confidence = 0\n",
    "            count = 0\n",
    "\n",
    "            # Ensure at least one detection exists\n",
    "            if len(idxs) > 0:\n",
    "                # Loop over the indexes we are keeping\n",
    "                for i in idxs.flatten():\n",
    "                    # Find positions\n",
    "                    centerX, centerY = centers[i][0], centers[i][1]\n",
    "\n",
    "                    if centerX <= W/3:\n",
    "                        W_pos = \"left \"\n",
    "                    elif centerX <= (W/3 * 2):\n",
    "                        W_pos = \"center \"\n",
    "                    else:\n",
    "                        W_pos = \"right \"\n",
    "\n",
    "                    if centerY <= H/3:\n",
    "                        H_pos = \"top \"\n",
    "                    elif centerY <= (H/3 * 2):\n",
    "                        H_pos = \"mid \"\n",
    "                    else:\n",
    "                        H_pos = \"bottom \"\n",
    "\n",
    "                    confidence = confidences[i]\n",
    "                    texts.append(f\"{H_pos} {W_pos} {LABELS[classIDs[i]]} with {confidence:.2f} accuracy, from x axis is {round(centerX*0.01)} meters, from y axis is {round(centerY*0.01)} meters\")\n",
    "                    total_confidence += confidence\n",
    "                    count += 1\n",
    "\n",
    "            if count > 0:\n",
    "                avg_confidence = total_confidence / count\n",
    "                \n",
    "            else:\n",
    "                avg_confidence = 0\n",
    "\n",
    "            print(f\"Detections: {texts}\")\n",
    "            print(f\"Average Accuracy: {avg_confidence*100:.2f}\")\n",
    "\n",
    "            if texts:\n",
    "                description = ', '.join(texts)\n",
    "                tts = gTTS(description, lang='en')\n",
    "                tts.save('tts.mp3')\n",
    "                tts_audio = AudioSegment.from_mp3(\"tts.mp3\")\n",
    "                subprocess.call([\"ffplay\", \"-nodisp\", \"-autoexit\", \"tts.mp3\"])\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "#os.remove(\"tts.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c211444f-f484-44c2-be46-0679c609e5c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
